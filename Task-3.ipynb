{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"Task-3.ipynb","version":"0.3.2","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZCGt0n1-84yH","colab_type":"text"},"source":["# Task 3:"]},{"cell_type":"markdown","metadata":{"id":"p_6okx7284yL","colab_type":"text"},"source":["Train a convolutional neural network on the SVHN dataset (http://ufldl.stanford.edu/housenumbers/) in format 2 (single digit classification). You should achieve at least 85% test-set accuracy with a base model. Also build a model using batch normalization. "]},{"cell_type":"markdown","metadata":{"id":"YaLm8jxX84yN","colab_type":"text"},"source":["### The Street View House Numbers (SVHN) Dataset"]},{"cell_type":"markdown","metadata":{"id":"MdEaVTW884yP","colab_type":"text"},"source":["SVHN is obtained from house numbers in Google Street View images. SVHN is a real-world image dataset for developing machine learning and object recognition algorithms with minimal requirement on data preprocessing and formatting. 10 classes, 1 for each digit. Digit '1' has label 1, '9' has label 9 and '0' has label 10.\n","73257 digits for training, 26032 digits for testing, and 531131 additional, somewhat less difficult samples, to use as extra training data\n","Comes in two formats:\n","1. Original images with character level bounding boxes.\n","2. MNIST-like 32-by-32 images centered around a single character (many of the images do contain some distractors at the sides).\n","\n"]},{"cell_type":"code","metadata":{"id":"qytgsiBg84yR","colab_type":"code","colab":{}},"source":["# Importing libraries\n","\n","import numpy as np\n","import pandas as pd\n","import tensorflow\n","from keras.layers import BatchNormalization\n","from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n","from keras.layers import Conv2D, MaxPooling2D, Flatten\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation\n","import keras\n","import tempfile\n","import scipy\n","import scipy.io as sio\n","import os\n"," "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s96PREBS84yY","colab_type":"code","colab":{}},"source":["# Importing dataset\n","\n","train = scipy.io.loadmat('train_32x32.mat')\n","test = scipy.io.loadmat('test_32x32.mat')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Boj2RgWR84yd","colab_type":"code","colab":{},"outputId":"b33d04e4-69d7-4d2a-cd8b-d59b05ec3646"},"source":["train"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Mon Dec  5 21:09:26 2011',\n"," '__version__': '1.0',\n"," '__globals__': [],\n"," 'X': array([[[[ 33,  84,  19, ...,  92, 190, 216],\n","          [ 30,  76,  54, ...,  78, 188, 217],\n","          [ 38,  59, 110, ..., 101, 191, 212]],\n"," \n","         [[ 15,  86,  20, ...,  94, 205, 221],\n","          [ 23,  73,  52, ...,  82, 203, 222],\n","          [ 19,  66, 111, ..., 105, 206, 217]],\n"," \n","         [[ 15,  77,  25, ..., 114, 220, 226],\n","          [ 17,  78,  57, ..., 101, 218, 227],\n","          [ 19,  56, 116, ..., 125, 220, 221]],\n"," \n","         ...,\n"," \n","         [[ 72,  90,  65, ..., 200, 229, 200],\n","          [ 65,  78, 144, ..., 201, 231, 199],\n","          [ 56,  69, 223, ..., 203, 224, 191]],\n"," \n","         [[ 82,  88,  78, ..., 192, 229, 193],\n","          [ 77,  77, 148, ..., 193, 229, 188],\n","          [ 57,  67, 218, ..., 195, 224, 182]],\n"," \n","         [[ 89,  88,  98, ..., 190, 229, 197],\n","          [ 79,  78, 158, ..., 191, 228, 189],\n","          [ 59,  66, 220, ..., 193, 223, 186]]],\n"," \n"," \n","        [[[ 28,  85,  21, ...,  92, 183, 204],\n","          [ 39,  77,  53, ...,  78, 182, 205],\n","          [ 35,  61, 110, ..., 103, 186, 202]],\n"," \n","         [[ 14,  83,  19, ...,  93, 200, 210],\n","          [ 25,  73,  52, ...,  80, 199, 211],\n","          [ 22,  64, 106, ..., 106, 201, 208]],\n"," \n","         [[ 14,  74,  25, ..., 111, 218, 220],\n","          [ 20,  69,  56, ...,  98, 217, 221],\n","          [ 17,  59, 111, ..., 124, 218, 217]],\n"," \n","         ...,\n"," \n","         [[ 40,  89,  63, ..., 181, 227, 201],\n","          [ 39,  82, 137, ..., 180, 228, 199],\n","          [ 50,  64, 208, ..., 184, 223, 193]],\n"," \n","         [[ 67,  88,  91, ..., 177, 227, 195],\n","          [ 58,  79, 153, ..., 176, 226, 191],\n","          [ 52,  70, 214, ..., 180, 222, 186]],\n"," \n","         [[ 83,  88, 130, ..., 183, 228, 196],\n","          [ 78,  81, 180, ..., 182, 224, 190],\n","          [ 60,  67, 229, ..., 187, 221, 186]]],\n"," \n"," \n","        [[[ 40,  83,  21, ...,  99, 171, 198],\n","          [ 41,  76,  53, ...,  84, 170, 198],\n","          [ 38,  60, 110, ..., 112, 175, 197]],\n"," \n","         [[ 18,  78,  20, ...,  94, 189, 202],\n","          [ 21,  77,  51, ...,  81, 189, 202],\n","          [ 26,  58, 106, ..., 110, 193, 201]],\n"," \n","         [[ 16,  61,  22, ..., 107, 213, 212],\n","          [ 17,  50,  52, ...,  94, 213, 211],\n","          [ 23,  54, 106, ..., 123, 215, 210]],\n"," \n","         ...,\n"," \n","         [[ 23,  90,  79, ..., 167, 231, 203],\n","          [ 29,  85, 147, ..., 166, 230, 200],\n","          [ 45,  63, 210, ..., 171, 226, 196]],\n"," \n","         [[ 35,  88, 125, ..., 172, 229, 198],\n","          [ 42,  83, 181, ..., 171, 226, 194],\n","          [ 44,  66, 230, ..., 176, 223, 191]],\n"," \n","         [[ 72,  85, 178, ..., 185, 227, 195],\n","          [ 69,  82, 218, ..., 184, 223, 190],\n","          [ 53,  70, 254, ..., 189, 220, 187]]],\n"," \n"," \n","        ...,\n"," \n"," \n","        [[[ 86, 100,  88, ...,  99, 187, 233],\n","          [ 81,  98, 162, ...,  94, 185, 226],\n","          [ 75,  72, 237, ..., 110, 186, 228]],\n"," \n","         [[ 87,  98,  89, ...,  96, 204, 230],\n","          [ 82,  94, 163, ...,  91, 202, 224],\n","          [ 71,  76, 238, ..., 109, 199, 225]],\n"," \n","         [[ 82,  95,  84, ..., 108, 217, 228],\n","          [ 79,  93, 156, ..., 103, 217, 223],\n","          [ 65,  73, 230, ..., 124, 210, 221]],\n"," \n","         ...,\n"," \n","         [[104, 104,  62, ..., 210, 204, 198],\n","          [104, 104, 142, ..., 207, 200, 196],\n","          [ 87,  86, 227, ..., 204, 195, 190]],\n"," \n","         [[104, 102,  67, ..., 206, 196, 184],\n","          [105, 102, 144, ..., 202, 193, 183],\n","          [ 81,  87, 226, ..., 200, 189, 177]],\n"," \n","         [[103, 100,  74, ..., 203, 196, 189],\n","          [105, 101, 145, ..., 197, 193, 187],\n","          [ 78,  78, 225, ..., 199, 189, 182]]],\n"," \n"," \n","        [[[ 84, 103,  88, ...,  94, 186, 231],\n","          [ 86, 104, 164, ...,  91, 184, 226],\n","          [ 64,  79, 240, ..., 103, 185, 228]],\n"," \n","         [[ 86, 106,  87, ...,  94, 198, 229],\n","          [ 79, 104, 160, ...,  91, 197, 224],\n","          [ 72,  79, 237, ..., 104, 194, 225]],\n"," \n","         [[ 82, 103,  88, ..., 110, 211, 227],\n","          [ 76, 103, 159, ..., 107, 211, 223],\n","          [ 72,  87, 237, ..., 121, 204, 222]],\n"," \n","         ...,\n"," \n","         [[110, 103,  60, ..., 219, 222, 195],\n","          [103, 104, 141, ..., 218, 216, 194],\n","          [ 84,  86, 230, ..., 215, 212, 186]],\n"," \n","         [[106, 103,  61, ..., 218, 214, 181],\n","          [105, 103, 141, ..., 215, 209, 181],\n","          [ 85,  87, 228, ..., 212, 205, 173]],\n"," \n","         [[106, 105,  65, ..., 212, 208, 186],\n","          [104,  99, 143, ..., 209, 205, 183],\n","          [ 86,  81, 226, ..., 209, 200, 177]]],\n"," \n"," \n","        [[[ 85, 103,  84, ...,  88, 190, 230],\n","          [ 88, 106, 160, ...,  87, 188, 226],\n","          [ 68,  82, 238, ...,  94, 190, 227]],\n"," \n","         [[ 89, 103,  81, ...,  85, 199, 230],\n","          [ 82, 105, 154, ...,  84, 197, 226],\n","          [ 72,  87, 233, ...,  93, 194, 227]],\n"," \n","         [[ 85, 104,  87, ..., 105, 208, 229],\n","          [ 79, 106, 158, ..., 103, 208, 225],\n","          [ 67,  91, 238, ..., 114, 201, 226]],\n"," \n","         ...,\n"," \n","         [[111, 113,  63, ..., 217, 232, 190],\n","          [104, 103, 144, ..., 217, 227, 190],\n","          [ 87,  88, 235, ..., 214, 223, 181]],\n"," \n","         [[109, 104,  62, ..., 221, 226, 178],\n","          [105, 104, 143, ..., 220, 221, 177],\n","          [ 86,  88, 232, ..., 219, 216, 169]],\n"," \n","         [[103, 103,  63, ..., 218, 218, 181],\n","          [106,  98, 145, ..., 217, 213, 178],\n","          [ 79,  80, 231, ..., 218, 209, 171]]]], dtype=uint8),\n"," 'y': array([[1],\n","        [9],\n","        [2],\n","        ...,\n","        [1],\n","        [6],\n","        [9]], dtype=uint8)}"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"rSLJSPUy84yl","colab_type":"code","colab":{},"outputId":"b8a1f7dd-28dd-44ed-fd29-3f41262b0f92"},"source":["test"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Mon Dec  5 21:18:15 2011',\n"," '__version__': '1.0',\n"," '__globals__': [],\n"," 'X': array([[[[ 38, 129, 150, ..., 115,  96, 101],\n","          [103, 142, 160, ..., 132,  65,  75],\n","          [ 60, 153, 169, ..., 142,  47,  60]],\n"," \n","         [[ 39, 127, 150, ..., 116,  97, 100],\n","          [104, 143, 163, ..., 133,  65,  73],\n","          [ 61, 152, 170, ..., 143,  49,  60]],\n"," \n","         [[ 39, 125, 152, ..., 117,  97,  99],\n","          [104, 143, 168, ..., 134,  65,  71],\n","          [ 62, 151, 172, ..., 144,  50,  59]],\n"," \n","         ...,\n"," \n","         [[ 41, 121, 153, ..., 114,  86,  95],\n","          [102, 133, 172, ..., 136,  61,  63],\n","          [ 61, 153, 180, ..., 141,  54,  52]],\n"," \n","         [[ 42, 123, 150, ..., 114,  87,  95],\n","          [103, 134, 171, ..., 136,  62,  63],\n","          [ 62, 156, 181, ..., 142,  55,  52]],\n"," \n","         [[ 39, 123, 147, ..., 115,  88,  96],\n","          [ 97, 135, 169, ..., 136,  63,  64],\n","          [ 57, 157, 180, ..., 143,  56,  51]]],\n"," \n"," \n","        [[[ 39, 134, 150, ..., 122,  96, 109],\n","          [104, 150, 160, ..., 139,  65,  84],\n","          [ 61, 160, 169, ..., 148,  48,  69]],\n"," \n","         [[ 39, 133, 151, ..., 123,  97, 107],\n","          [104, 149, 163, ..., 140,  65,  80],\n","          [ 61, 158, 169, ..., 149,  50,  67]],\n"," \n","         [[ 39, 132, 153, ..., 124,  97, 104],\n","          [104, 149, 168, ..., 141,  65,  76],\n","          [ 62, 157, 171, ..., 150,  51,  64]],\n"," \n","         ...,\n"," \n","         [[ 41, 127, 156, ..., 118,  85,  95],\n","          [102, 138, 174, ..., 139,  62,  62],\n","          [ 61, 157, 180, ..., 143,  53,  53]],\n"," \n","         [[ 43, 126, 153, ..., 118,  87,  93],\n","          [101, 137, 171, ..., 139,  63,  61],\n","          [ 63, 157, 180, ..., 145,  54,  50]],\n"," \n","         [[ 39, 125, 150, ..., 119,  88,  93],\n","          [ 97, 137, 169, ..., 139,  64,  61],\n","          [ 57, 158, 179, ..., 146,  55,  49]]],\n"," \n"," \n","        [[[ 38, 141, 144, ..., 131,  96, 117],\n","          [105, 158, 154, ..., 148,  64,  95],\n","          [ 62, 168, 163, ..., 155,  49,  78]],\n"," \n","         [[ 37, 140, 146, ..., 132,  96, 114],\n","          [104, 157, 157, ..., 149,  64,  90],\n","          [ 61, 166, 163, ..., 156,  50,  75]],\n"," \n","         [[ 39, 140, 148, ..., 133,  96, 108],\n","          [106, 157, 162, ..., 149,  64,  83],\n","          [ 63, 165, 165, ..., 157,  51,  70]],\n"," \n","         ...,\n"," \n","         [[ 43, 135, 157, ..., 125,  87,  95],\n","          [101, 147, 172, ..., 143,  66,  62],\n","          [ 63, 163, 177, ..., 147,  54,  53]],\n"," \n","         [[ 43, 132, 153, ..., 125,  90,  92],\n","          [100, 143, 168, ..., 143,  68,  60],\n","          [ 64, 161, 175, ..., 149,  57,  50]],\n"," \n","         [[ 39, 130, 150, ..., 126,  91,  91],\n","          [ 97, 141, 165, ..., 144,  69,  59],\n","          [ 59, 160, 173, ..., 150,  58,  48]]],\n"," \n"," \n","        ...,\n"," \n"," \n","        [[[ 52, 147, 135, ...,  94,  99, 116],\n","          [119, 161, 150, ...,  97,  68,  96],\n","          [ 78, 174, 167, ..., 114,  48,  85]],\n"," \n","         [[ 53, 142, 134, ...,  93,  99, 116],\n","          [120, 156, 149, ...,  96,  70,  96],\n","          [ 79, 169, 166, ..., 113,  50,  87]],\n"," \n","         [[ 51, 136, 132, ...,  92, 100, 115],\n","          [118, 151, 147, ...,  95,  73,  95],\n","          [ 77, 163, 165, ..., 112,  53,  87]],\n"," \n","         ...,\n"," \n","         [[ 44,  93, 124, ..., 140, 138, 101],\n","          [118,  97, 140, ..., 153, 121,  70],\n","          [ 69, 126, 157, ..., 162, 105,  50]],\n"," \n","         [[ 44,  96, 122, ..., 142, 127, 101],\n","          [117, 100, 139, ..., 155, 109,  70],\n","          [ 71, 128, 157, ..., 164,  93,  50]],\n"," \n","         [[ 41, 104, 121, ..., 144, 117, 101],\n","          [114, 109, 138, ..., 157,  98,  70],\n","          [ 69, 136, 158, ..., 165,  83,  50]]],\n"," \n"," \n","        [[[ 50, 138, 147, ..., 117,  98, 104],\n","          [117, 152, 165, ..., 122,  67,  83],\n","          [ 76, 165, 179, ..., 137,  46,  71]],\n"," \n","         [[ 51, 130, 147, ..., 117,  99, 105],\n","          [118, 144, 164, ..., 122,  70,  84],\n","          [ 77, 157, 179, ..., 137,  48,  73]],\n"," \n","         [[ 49, 121, 147, ..., 118, 100, 105],\n","          [116, 136, 164, ..., 123,  73,  84],\n","          [ 75, 148, 179, ..., 137,  51,  74]],\n"," \n","         ...,\n"," \n","         [[ 44,  93, 139, ..., 141, 126, 101],\n","          [117,  99, 155, ..., 154, 108,  70],\n","          [ 71, 128, 170, ..., 163,  92,  49]],\n"," \n","         [[ 45, 101, 139, ..., 143, 118, 102],\n","          [116, 107, 155, ..., 156,  99,  71],\n","          [ 72, 134, 170, ..., 164,  83,  50]],\n"," \n","         [[ 42, 112, 138, ..., 144, 110, 102],\n","          [113, 118, 155, ..., 157,  91,  71],\n","          [ 69, 144, 171, ..., 165,  75,  50]]],\n"," \n"," \n","        [[[ 48, 131, 159, ..., 132,  97,  95],\n","          [115, 145, 177, ..., 138,  66,  73],\n","          [ 74, 158, 189, ..., 152,  45,  62]],\n"," \n","         [[ 48, 122, 159, ..., 133,  99,  96],\n","          [115, 136, 177, ..., 140,  69,  74],\n","          [ 74, 149, 190, ..., 152,  48,  63]],\n"," \n","         [[ 46, 111, 159, ..., 135, 100,  97],\n","          [113, 126, 177, ..., 142,  72,  75],\n","          [ 72, 139, 191, ..., 153,  51,  64]],\n"," \n","         ...,\n"," \n","         [[ 43, 101, 152, ..., 142, 118, 101],\n","          [116, 108, 168, ..., 155,  99,  70],\n","          [ 71, 135, 181, ..., 164,  82,  49]],\n"," \n","         [[ 44, 111, 152, ..., 143, 111, 102],\n","          [115, 118, 168, ..., 156,  92,  71],\n","          [ 71, 144, 182, ..., 165,  74,  50]],\n"," \n","         [[ 42, 123, 152, ..., 144, 104, 102],\n","          [113, 130, 168, ..., 157,  85,  71],\n","          [ 71, 156, 183, ..., 165,  68,  50]]]], dtype=uint8),\n"," 'y': array([[5],\n","        [2],\n","        [1],\n","        ...,\n","        [7],\n","        [6],\n","        [7]], dtype=uint8)}"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"ZcPvZ6ot84yr","colab_type":"code","colab":{}},"source":["x_train = train['X']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XW5kNOIQ84yw","colab_type":"code","colab":{}},"source":["y_train = train['y']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Iy1wwt3784y2","colab_type":"code","colab":{}},"source":["x_test = test['X']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VwQqJ7m984y7","colab_type":"code","colab":{}},"source":["y_test = test['y']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"91DPEOxr84y_","colab_type":"code","colab":{}},"source":["batch_size = 128\n","num_classes = 11\n","epochs = 12"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mb1FE9B984zE","colab_type":"code","colab":{}},"source":["# Input image dimensions\n","img_rows, img_cols = 32, 32"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"92L5no2C84zI","colab_type":"code","colab":{}},"source":["# Data, shuffled and split between train and test sets\n","\n","x_train=np.rollaxis(x_train, 3, 0)\n","x_test=np.rollaxis(x_test,3,0)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V9O4esH_84zM","colab_type":"code","colab":{}},"source":["# Defining the input shape\n","input_shape = (img_rows, img_cols, 3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P6nklKU584zO","colab_type":"code","colab":{}},"source":["from keras.utils.np_utils import to_categorical\n","\n","y_train_binary = to_categorical(y_train)\n","y_test_xbinary=to_categorical(y_test)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-BUASt4B84zQ","colab_type":"code","colab":{}},"source":["# Implementing the Base model\n","\n","num_classes = 11\n","cnn = Sequential()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YYzMUJpn84zS","colab_type":"code","colab":{}},"source":["cnn.add(Conv2D(16, kernel_size=(5, 5),\n","                 activation='relu',\n","                 input_shape=input_shape))\n","cnn.add(MaxPooling2D(pool_size=(2, 2)))\n","cnn.add(Conv2D(32, (5, 5),activation='relu'))\n","cnn.add(MaxPooling2D(pool_size=(2, 2)))\n","cnn.add(Conv2D(64, (5, 5),activation='relu'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6anui8_c84zU","colab_type":"code","colab":{}},"source":["cnn.add(Flatten())\n","cnn.add(Dropout(0.5))\n","cnn.add(Dense(16, activation='relu'))\n","cnn.add(Dense(num_classes, activation='softmax'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xt3PJfKO84zW","colab_type":"code","colab":{}},"source":["cnn.compile(\"adam\", \"categorical_crossentropy\", metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tMvRBs_e84zY","colab_type":"code","colab":{},"outputId":"dd71f38d-df00-4739-cb09-7a23954b9129"},"source":["# Defining history by fitting cnn\n","\n","history_cnn = cnn.fit(x_train, y_train_binary,\n","                      batch_size = 200, epochs=50, verbose=1, validation_split=.2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 58605 samples, validate on 14652 samples\n","Epoch 1/50\n","58605/58605 [==============================] - 122s 2ms/step - loss: 2.0201 - acc: 0.3310 - val_loss: 1.1757 - val_acc: 0.6490\n","Epoch 2/50\n","58605/58605 [==============================] - 110s 2ms/step - loss: 1.1658 - acc: 0.6210 - val_loss: 0.8290 - val_acc: 0.7519\n","Epoch 3/50\n","58605/58605 [==============================] - 108s 2ms/step - loss: 0.9437 - acc: 0.6994 - val_loss: 0.6419 - val_acc: 0.8168\n","Epoch 4/50\n","58605/58605 [==============================] - 111s 2ms/step - loss: 0.8494 - acc: 0.7327 - val_loss: 0.5768 - val_acc: 0.8356\n","Epoch 5/50\n","58605/58605 [==============================] - 114s 2ms/step - loss: 0.7746 - acc: 0.7557 - val_loss: 0.5796 - val_acc: 0.8346\n","Epoch 6/50\n","58605/58605 [==============================] - 111s 2ms/step - loss: 0.7375 - acc: 0.7671 - val_loss: 0.5328 - val_acc: 0.8490\n","Epoch 7/50\n","58605/58605 [==============================] - 108s 2ms/step - loss: 0.6988 - acc: 0.7793 - val_loss: 0.5213 - val_acc: 0.8479\n","Epoch 8/50\n","58605/58605 [==============================] - 101s 2ms/step - loss: 0.6805 - acc: 0.7867 - val_loss: 0.5148 - val_acc: 0.8535\n","Epoch 9/50\n","58605/58605 [==============================] - 102s 2ms/step - loss: 0.6737 - acc: 0.7881 - val_loss: 0.5306 - val_acc: 0.8452\n","Epoch 10/50\n","58605/58605 [==============================] - 108s 2ms/step - loss: 0.6709 - acc: 0.7865 - val_loss: 0.5298 - val_acc: 0.8455\n","Epoch 11/50\n","58605/58605 [==============================] - 104s 2ms/step - loss: 0.6367 - acc: 0.7999 - val_loss: 0.5572 - val_acc: 0.8423\n","Epoch 12/50\n","58605/58605 [==============================] - 106s 2ms/step - loss: 0.6239 - acc: 0.8015 - val_loss: 0.5043 - val_acc: 0.8583\n","Epoch 13/50\n","58605/58605 [==============================] - 107s 2ms/step - loss: 0.6109 - acc: 0.8077 - val_loss: 0.4865 - val_acc: 0.8619\n","Epoch 14/50\n","58605/58605 [==============================] - 107s 2ms/step - loss: 0.5967 - acc: 0.8125 - val_loss: 0.4949 - val_acc: 0.8600\n","Epoch 15/50\n","58605/58605 [==============================] - 111s 2ms/step - loss: 0.5797 - acc: 0.8157 - val_loss: 0.5012 - val_acc: 0.8592\n","Epoch 16/50\n","58605/58605 [==============================] - 106s 2ms/step - loss: 0.5810 - acc: 0.8188 - val_loss: 0.5083 - val_acc: 0.8600\n","Epoch 17/50\n","58605/58605 [==============================] - 108s 2ms/step - loss: 0.5678 - acc: 0.8212 - val_loss: 0.4901 - val_acc: 0.8639\n","Epoch 18/50\n","58605/58605 [==============================] - 105s 2ms/step - loss: 0.5643 - acc: 0.8215 - val_loss: 0.4666 - val_acc: 0.8645\n","Epoch 19/50\n","58605/58605 [==============================] - 106s 2ms/step - loss: 0.5507 - acc: 0.8277 - val_loss: 0.4705 - val_acc: 0.8696\n","Epoch 20/50\n","58605/58605 [==============================] - 104s 2ms/step - loss: 0.5414 - acc: 0.8305 - val_loss: 0.4775 - val_acc: 0.8648\n","Epoch 21/50\n","58605/58605 [==============================] - 104s 2ms/step - loss: 0.5341 - acc: 0.8318 - val_loss: 0.4591 - val_acc: 0.8741\n","Epoch 22/50\n","58605/58605 [==============================] - 115s 2ms/step - loss: 0.5349 - acc: 0.8306 - val_loss: 0.4480 - val_acc: 0.8735\n","Epoch 23/50\n","58605/58605 [==============================] - 124s 2ms/step - loss: 0.5285 - acc: 0.8329 - val_loss: 0.4624 - val_acc: 0.8715\n","Epoch 24/50\n","58605/58605 [==============================] - 109s 2ms/step - loss: 0.5225 - acc: 0.8365 - val_loss: 0.4599 - val_acc: 0.8703\n","Epoch 25/50\n","58605/58605 [==============================] - 105s 2ms/step - loss: 0.5158 - acc: 0.8377 - val_loss: 0.4546 - val_acc: 0.8722\n","Epoch 26/50\n","58605/58605 [==============================] - 106s 2ms/step - loss: 0.5030 - acc: 0.8417 - val_loss: 0.4721 - val_acc: 0.8667\n","Epoch 27/50\n","58605/58605 [==============================] - 106s 2ms/step - loss: 0.5059 - acc: 0.8410 - val_loss: 0.4771 - val_acc: 0.8704\n","Epoch 28/50\n","58605/58605 [==============================] - 113s 2ms/step - loss: 0.5065 - acc: 0.8403 - val_loss: 0.4435 - val_acc: 0.8744\n","Epoch 29/50\n","58605/58605 [==============================] - 109s 2ms/step - loss: 0.4950 - acc: 0.8439 - val_loss: 0.4537 - val_acc: 0.8729\n","Epoch 30/50\n","58605/58605 [==============================] - 112s 2ms/step - loss: 0.4848 - acc: 0.8471 - val_loss: 0.4611 - val_acc: 0.8668\n","Epoch 31/50\n","58605/58605 [==============================] - 139s 2ms/step - loss: 0.4901 - acc: 0.8452 - val_loss: 0.4460 - val_acc: 0.8749\n","Epoch 32/50\n","58605/58605 [==============================] - 122s 2ms/step - loss: 0.4768 - acc: 0.8504 - val_loss: 0.4571 - val_acc: 0.8722\n","Epoch 33/50\n","58605/58605 [==============================] - 118s 2ms/step - loss: 0.4823 - acc: 0.8483 - val_loss: 0.4549 - val_acc: 0.8708\n","Epoch 34/50\n","58605/58605 [==============================] - 115s 2ms/step - loss: 0.4734 - acc: 0.8506 - val_loss: 0.4489 - val_acc: 0.8708\n","Epoch 35/50\n","58605/58605 [==============================] - 109s 2ms/step - loss: 0.4742 - acc: 0.8508 - val_loss: 0.4529 - val_acc: 0.8740\n","Epoch 36/50\n","58605/58605 [==============================] - 102s 2ms/step - loss: 0.4603 - acc: 0.8552 - val_loss: 0.4653 - val_acc: 0.8742\n","Epoch 37/50\n","58605/58605 [==============================] - 101s 2ms/step - loss: 0.4637 - acc: 0.8548 - val_loss: 0.4534 - val_acc: 0.8746\n","Epoch 38/50\n","58605/58605 [==============================] - 101s 2ms/step - loss: 0.4546 - acc: 0.8569 - val_loss: 0.4691 - val_acc: 0.8728\n","Epoch 39/50\n","58605/58605 [==============================] - 102s 2ms/step - loss: 0.4604 - acc: 0.8550 - val_loss: 0.4466 - val_acc: 0.8756\n","Epoch 40/50\n","58605/58605 [==============================] - 102s 2ms/step - loss: 0.4404 - acc: 0.8611 - val_loss: 0.4539 - val_acc: 0.8744\n","Epoch 41/50\n","58605/58605 [==============================] - 101s 2ms/step - loss: 0.4450 - acc: 0.8594 - val_loss: 0.4418 - val_acc: 0.8777\n","Epoch 42/50\n","58605/58605 [==============================] - 101s 2ms/step - loss: 0.4440 - acc: 0.8614 - val_loss: 0.4801 - val_acc: 0.8706\n","Epoch 43/50\n","58605/58605 [==============================] - 104s 2ms/step - loss: 0.4466 - acc: 0.8607 - val_loss: 0.4502 - val_acc: 0.8756\n","Epoch 44/50\n","58605/58605 [==============================] - 105s 2ms/step - loss: 0.4419 - acc: 0.8617 - val_loss: 0.4409 - val_acc: 0.8767\n","Epoch 45/50\n","58605/58605 [==============================] - 104s 2ms/step - loss: 0.4356 - acc: 0.8627 - val_loss: 0.4317 - val_acc: 0.8806\n","Epoch 46/50\n","58605/58605 [==============================] - 105s 2ms/step - loss: 0.4322 - acc: 0.8644 - val_loss: 0.4373 - val_acc: 0.8789\n","Epoch 47/50\n","58605/58605 [==============================] - 105s 2ms/step - loss: 0.4315 - acc: 0.8666 - val_loss: 0.4491 - val_acc: 0.8762\n","Epoch 48/50\n","58605/58605 [==============================] - 106s 2ms/step - loss: 0.4280 - acc: 0.8653 - val_loss: 0.4416 - val_acc: 0.8784\n","Epoch 49/50\n","58605/58605 [==============================] - 105s 2ms/step - loss: 0.4234 - acc: 0.8668 - val_loss: 0.4339 - val_acc: 0.8789\n","Epoch 50/50\n","58605/58605 [==============================] - 102s 2ms/step - loss: 0.4229 - acc: 0.8662 - val_loss: 0.4352 - val_acc: 0.8795\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DPPegD2i84za","colab_type":"code","colab":{},"outputId":"090656e3-514f-4166-8dcd-7cafce4ce04f"},"source":["# Getting score for the base model\n","\n","score_basic = cnn.evaluate(x_test, y_test_xbinary)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["26032/26032 [==============================] - 18s 702us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NOynVewQ84ze","colab_type":"code","colab":{},"outputId":"fc690fa0-0b35-45e7-b516-751d76283aea"},"source":["# Printing the base model test accuracy\n","\n","print(\"Task3 base model Test Accuracy: {:.2f}\".format(score_basic[1]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Task3 base model Test Accuracy: 0.87\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"s4M0OKSm84zh","colab_type":"code","colab":{}},"source":["# Batch Normalization\n","\n","# Tiny convent with batch normalization\n","\n","num_classes = 11\n","cnn_small_bn = Sequential()\n","cnn_small_bn.add(Conv2D(8, kernel_size=(3, 3),\n","                 input_shape=input_shape))\n","cnn_small_bn.add(Activation(\"relu\"))\n","cnn_small_bn.add(BatchNormalization())\n","cnn_small_bn.add(MaxPooling2D(pool_size=(2, 2)))\n","cnn_small_bn.add(Conv2D(8, (3, 3)))\n","cnn_small_bn.add(Activation(\"relu\"))\n","cnn_small_bn.add(BatchNormalization())\n","cnn_small_bn.add(MaxPooling2D(pool_size=(2, 2)))\n","cnn_small_bn.add(Flatten())\n","cnn_small_bn.add(Dense(64, activation='relu'))\n","cnn_small_bn.add(Dense(num_classes, activation='softmax'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vYW0mMCW84zj","colab_type":"code","colab":{}},"source":["cnn_small_bn.compile(\"adam\", \"categorical_crossentropy\", metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"38bqRczE84zl","colab_type":"code","colab":{},"outputId":"48963bb4-dcad-44a1-d1a7-76eebf47c0ea"},"source":["history_cnn_small_bn = cnn_small_bn.fit(x_train, y_train_binary,\n","                                        batch_size=200, epochs=50, verbose=1, validation_split=.2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 58605 samples, validate on 14652 samples\n","Epoch 1/50\n","58605/58605 [==============================] - 103s 2ms/step - loss: 1.5218 - acc: 0.5090 - val_loss: 0.8433 - val_acc: 0.7388\n","Epoch 2/50\n","58605/58605 [==============================] - 102s 2ms/step - loss: 0.6431 - acc: 0.8109 - val_loss: 0.6373 - val_acc: 0.8134\n","Epoch 3/50\n","58605/58605 [==============================] - 102s 2ms/step - loss: 0.5251 - acc: 0.8474 - val_loss: 0.7690 - val_acc: 0.7632\n","Epoch 4/50\n","58605/58605 [==============================] - 102s 2ms/step - loss: 0.4799 - acc: 0.8609 - val_loss: 0.5007 - val_acc: 0.8522\n","Epoch 5/50\n","58605/58605 [==============================] - 104s 2ms/step - loss: 0.4293 - acc: 0.8745 - val_loss: 0.5059 - val_acc: 0.8521\n","Epoch 6/50\n","58605/58605 [==============================] - 107s 2ms/step - loss: 0.4042 - acc: 0.8813 - val_loss: 0.4756 - val_acc: 0.8607\n","Epoch 7/50\n","58605/58605 [==============================] - 107s 2ms/step - loss: 0.3788 - acc: 0.8887 - val_loss: 0.5297 - val_acc: 0.8406\n","Epoch 8/50\n","58605/58605 [==============================] - 107s 2ms/step - loss: 0.3645 - acc: 0.8928 - val_loss: 0.4930 - val_acc: 0.8535\n","Epoch 9/50\n","58605/58605 [==============================] - 107s 2ms/step - loss: 0.3456 - acc: 0.8982 - val_loss: 0.4543 - val_acc: 0.8679\n","Epoch 10/50\n","58605/58605 [==============================] - 108s 2ms/step - loss: 0.3295 - acc: 0.9016 - val_loss: 0.5341 - val_acc: 0.8405\n","Epoch 11/50\n","58605/58605 [==============================] - 105s 2ms/step - loss: 0.3261 - acc: 0.9029 - val_loss: 0.4524 - val_acc: 0.8698\n","Epoch 12/50\n","58605/58605 [==============================] - 107s 2ms/step - loss: 0.3113 - acc: 0.9079 - val_loss: 0.4401 - val_acc: 0.8751\n","Epoch 13/50\n","58605/58605 [==============================] - 107s 2ms/step - loss: 0.2968 - acc: 0.9110 - val_loss: 0.4456 - val_acc: 0.8696\n","Epoch 14/50\n","58605/58605 [==============================] - 106s 2ms/step - loss: 0.2947 - acc: 0.9111 - val_loss: 0.4345 - val_acc: 0.8777\n","Epoch 15/50\n","58605/58605 [==============================] - 106s 2ms/step - loss: 0.2790 - acc: 0.9178 - val_loss: 0.4876 - val_acc: 0.8648\n","Epoch 16/50\n","58605/58605 [==============================] - 107s 2ms/step - loss: 0.2765 - acc: 0.9177 - val_loss: 0.4715 - val_acc: 0.8660\n","Epoch 17/50\n","58605/58605 [==============================] - 622s 11ms/step - loss: 0.2693 - acc: 0.9195 - val_loss: 0.4547 - val_acc: 0.8735\n","Epoch 18/50\n","58605/58605 [==============================] - 106s 2ms/step - loss: 0.2593 - acc: 0.9230 - val_loss: 0.4460 - val_acc: 0.8751\n","Epoch 19/50\n","58605/58605 [==============================] - 105s 2ms/step - loss: 0.2532 - acc: 0.9250 - val_loss: 0.4858 - val_acc: 0.8626\n","Epoch 20/50\n","58605/58605 [==============================] - 102s 2ms/step - loss: 0.2557 - acc: 0.9237 - val_loss: 0.4577 - val_acc: 0.8769\n","Epoch 21/50\n","58605/58605 [==============================] - 104s 2ms/step - loss: 0.2408 - acc: 0.9282 - val_loss: 0.4660 - val_acc: 0.8756\n","Epoch 22/50\n","58605/58605 [==============================] - 102s 2ms/step - loss: 0.2374 - acc: 0.9299 - val_loss: 0.5333 - val_acc: 0.8612\n","Epoch 23/50\n","58605/58605 [==============================] - 104s 2ms/step - loss: 0.2438 - acc: 0.9262 - val_loss: 0.4700 - val_acc: 0.8742\n","Epoch 24/50\n","58605/58605 [==============================] - 108s 2ms/step - loss: 0.2301 - acc: 0.9310 - val_loss: 0.5170 - val_acc: 0.8617\n","Epoch 25/50\n","58605/58605 [==============================] - 107s 2ms/step - loss: 0.2292 - acc: 0.9314 - val_loss: 0.4859 - val_acc: 0.8707\n","Epoch 26/50\n","58605/58605 [==============================] - 107s 2ms/step - loss: 0.2166 - acc: 0.9361 - val_loss: 0.4843 - val_acc: 0.8720\n","Epoch 27/50\n","58605/58605 [==============================] - 107s 2ms/step - loss: 0.2166 - acc: 0.9361 - val_loss: 0.5101 - val_acc: 0.8653\n","Epoch 28/50\n","58605/58605 [==============================] - 106s 2ms/step - loss: 0.2119 - acc: 0.9370 - val_loss: 0.4939 - val_acc: 0.8758\n","Epoch 29/50\n","58605/58605 [==============================] - 104s 2ms/step - loss: 0.2113 - acc: 0.9368 - val_loss: 0.5151 - val_acc: 0.8661\n","Epoch 30/50\n","58605/58605 [==============================] - 102s 2ms/step - loss: 0.2023 - acc: 0.9392 - val_loss: 0.6037 - val_acc: 0.8525\n","Epoch 31/50\n","58605/58605 [==============================] - 102s 2ms/step - loss: 0.2216 - acc: 0.9336 - val_loss: 0.5044 - val_acc: 0.8700\n","Epoch 32/50\n","58605/58605 [==============================] - 102s 2ms/step - loss: 0.1955 - acc: 0.9422 - val_loss: 0.5308 - val_acc: 0.8687\n","Epoch 33/50\n","58605/58605 [==============================] - 102s 2ms/step - loss: 0.1983 - acc: 0.9397 - val_loss: 0.5223 - val_acc: 0.8701\n","Epoch 34/50\n","58605/58605 [==============================] - 104s 2ms/step - loss: 0.1963 - acc: 0.9418 - val_loss: 0.5251 - val_acc: 0.8718\n","Epoch 35/50\n","58605/58605 [==============================] - 105s 2ms/step - loss: 0.1876 - acc: 0.9442 - val_loss: 0.5335 - val_acc: 0.8707\n","Epoch 36/50\n","58605/58605 [==============================] - 107s 2ms/step - loss: 0.1835 - acc: 0.9460 - val_loss: 0.5639 - val_acc: 0.8632\n","Epoch 37/50\n","58605/58605 [==============================] - 106s 2ms/step - loss: 0.1818 - acc: 0.9462 - val_loss: 0.5459 - val_acc: 0.8686\n","Epoch 38/50\n","58605/58605 [==============================] - 103s 2ms/step - loss: 0.1792 - acc: 0.9472 - val_loss: 0.5638 - val_acc: 0.8669\n","Epoch 39/50\n","58605/58605 [==============================] - 103s 2ms/step - loss: 0.1784 - acc: 0.9461 - val_loss: 0.5714 - val_acc: 0.8649\n","Epoch 40/50\n","58605/58605 [==============================] - 102s 2ms/step - loss: 0.1774 - acc: 0.9469 - val_loss: 0.6098 - val_acc: 0.8575\n","Epoch 41/50\n","58605/58605 [==============================] - 102s 2ms/step - loss: 0.1924 - acc: 0.9411 - val_loss: 0.5600 - val_acc: 0.8682\n","Epoch 42/50\n","58605/58605 [==============================] - 104s 2ms/step - loss: 0.1708 - acc: 0.9497 - val_loss: 0.5844 - val_acc: 0.8673\n","Epoch 43/50\n","58605/58605 [==============================] - 102s 2ms/step - loss: 0.1693 - acc: 0.9497 - val_loss: 0.5734 - val_acc: 0.8653\n","Epoch 44/50\n","58605/58605 [==============================] - 107s 2ms/step - loss: 0.1647 - acc: 0.9518 - val_loss: 0.6217 - val_acc: 0.8552\n","Epoch 45/50\n","58605/58605 [==============================] - 108s 2ms/step - loss: 0.1762 - acc: 0.9466 - val_loss: 0.6173 - val_acc: 0.8640\n","Epoch 46/50\n","58605/58605 [==============================] - 106s 2ms/step - loss: 0.1809 - acc: 0.9451 - val_loss: 0.5842 - val_acc: 0.8643\n","Epoch 47/50\n","58605/58605 [==============================] - 108s 2ms/step - loss: 0.1579 - acc: 0.9539 - val_loss: 0.6030 - val_acc: 0.8619\n","Epoch 48/50\n","58605/58605 [==============================] - 108s 2ms/step - loss: 0.1727 - acc: 0.9470 - val_loss: 0.6010 - val_acc: 0.8677\n","Epoch 49/50\n","58605/58605 [==============================] - 107s 2ms/step - loss: 0.1575 - acc: 0.9534 - val_loss: 0.6258 - val_acc: 0.8608\n","Epoch 50/50\n","58605/58605 [==============================] - 107s 2ms/step - loss: 0.1521 - acc: 0.9552 - val_loss: 0.6034 - val_acc: 0.8684\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"807QZ0k984zo","colab_type":"code","colab":{},"outputId":"36163759-32be-4bdd-a639-968cd650acd4"},"source":["# Evaluating Score with batch normalization\n","\n","score_batch = cnn_small_bn.evaluate(x_test,y_test_xbinary)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["26032/26032 [==============================] - 14s 531us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Nukw2-HX84zr","colab_type":"code","colab":{},"outputId":"8c85cfd0-0a00-420c-facf-a2fc1e7eb609"},"source":["# Printing score means test accuracy of the batch model after batch normalization\n","\n","print(\"Task3 batch model Test Accuracy: {:.2f}\".format(score_batch[1]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Task3 batch model Test Accuracy: 0.85\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EMcJiUnu84zw","colab_type":"text"},"source":["After observing the behaviour of the models as we can see from the above results, the test accuracy of without batch normalization for the base model is 87% which is pretty awesome for the convolutional neural network for SVHN dataset and the test accuracy for the batch model after batch normalization is 85 %."]}]}